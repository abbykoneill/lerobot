{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3337bc89",
   "metadata": {},
   "source": [
    "# Introduction to Imitation Learning (IL)\n",
    "\n",
    "### Lab Table of Contents\n",
    "* **Part 1**\n",
    "    1. **[1_imitation_learning.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part_1/1_imitation_learning.ipynb)**\n",
    "* Part 2\n",
    "    1. [1_chatgpt.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/1_chatgpt.ipynb)\n",
    "    2. [2_CLIP.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/2_CLIP.ipynb)\n",
    "    3. [3_VLM_BLIP.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/3_VLM_BLIP.ipynb)\n",
    "    4. [4_VLA.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/4_VLA.ipynb)\n",
    "    5. [5_safety.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/5_safety.ipynb)\n",
    "* [Lab Checkoff](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/checkoff.txt)\n",
    "\n",
    "## Imitation Learning\n",
    "\n",
    "Imitation Learning is a machine learning approach that uses expert trajectories rather than a reward function to train an agent to perform a task. The expert trajectories can be from anything that already knows how to complete the specific tasks, i.e. a human, another robot, an AI system, etc.\n",
    "\n",
    "For this part of the lab, you will explore an interactive Imitation Learning activity for a simple robotics \"Pick-and-Place\" task using Behavioral Cloning.\n",
    "\n",
    "### In Lab Part 1, you will explore:\n",
    "1. Expert Demonstration (Data Collection)\n",
    "2. Policy Training (Supervised Learning)\n",
    "3. Policy Execution and Failure due to Covariate Shift (Testing)\n",
    "4. Interactive Correction and Retraining\n",
    "\n",
    "Follow the prompts in this notebook. Discuss all answers with your lab partner.\n",
    "\n",
    "#### Before Beginning with Code - Complete Environment Set-Up:\n",
    "* `conda create -n <env_name> python=3.10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058d94b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "\n",
    "!pip install ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a756a",
   "metadata": {},
   "source": [
    "### (A) Environment and Robot Setup\n",
    "\n",
    "To create a useful representation of the observed robot states, we can simplify the target task into a single dimension: i.e. **distance to target**. In this simplified model, we can describe the system of a \"Pick-and-Place\" task through a series of States and Actions where\n",
    "* `State S = [distance_to_target]`\n",
    "* `Action A = [movement_step]`\n",
    "\n",
    "We can then determine an explicit goal for the system. The task is complete when `distance_to_target` is close to zero.\n",
    "\n",
    "Run the following cell to set up the problem and establish the `get_robot_state` and `execute_action` functions.\n",
    "1. What might be a limitation of simplifying the task into a 1-dimensional distance to the target?\n",
    "2. What is one example of a desired robotic task that could be accurately represented with this simplifed model?\n",
    "3. What is one example of a desired robotic task that could not be accurately modeled by only looking at distance to target in one dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbc371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Environment and Robot Setup ---\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "TARGET_DISTANCE = 0.0 # The task is complete when distance is close to zero.\n",
    "\n",
    "def get_robot_state(current_position):\n",
    "    # Simulates getting the robot's current state (distance to target).\n",
    "    return np.array([current_position - TARGET_DISTANCE])\n",
    "\n",
    "def execute_action(current_position, action):\n",
    "    # Simulates the robot moving based on the action.\n",
    "    # The new position is the current position adjusted by the action (movement step)\n",
    "    new_position = current_position + action[0]\n",
    "    return new_position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dda79f",
   "metadata": {},
   "source": [
    "### (B) Demonstration (Data Collection - The Expert Phase)\n",
    "\n",
    "The first step in imitation learning is data collection from the expert demonstrations. These expert trajectories can be from any other system that can perform the desired task, i.e. human, another robot, an AI system, etc.\n",
    "\n",
    "Run the following cell to provide the model with expert trajectories.\n",
    "1. What is the intuition behind having incremental movements towards the target goal in the expert trajectories?\n",
    "2. Read the `expert_trajectory` dataset and each trajectory's annotation to understand the setup. Fill in `custom_expert_trajectory` to build your own expert dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29243e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Demonstration (Data Collection - The Expert Phase) ---\n",
    "\n",
    "# The Expert (the user) provides demonstrations for a successful run.\n",
    "print(\"--- Step 1: Expert Demonstration (Data Collection) ---\")\n",
    "print(\"The Expert (user) provides successful movement trajectories to reach the target (10.0 -> 0.0).\")\n",
    "\n",
    "# The Expert dataset D = [(State, Action), ...]\n",
    "# Adjusted with more fine-grained steps near the end for better convergence\n",
    "expert_trajectory = [\n",
    "    (get_robot_state(10.0), np.array([-2.0])), # At dist 10, move -2.0\n",
    "    (get_robot_state(8.0), np.array([-1.5])),  # At dist 8, move -1.5\n",
    "    (get_robot_state(6.5), np.array([-1.0])),  # At dist 6.5, move -1.0\n",
    "    (get_robot_state(5.5), np.array([-1.0])),  # At dist 5.5, move -1.0\n",
    "    (get_robot_state(4.5), np.array([-0.5])),  # At dist 4.5, move -0.5\n",
    "    (get_robot_state(4.0), np.array([-0.5])),  # At dist 4.0, move -0.5\n",
    "    (get_robot_state(3.5), np.array([-0.5])),  # At dist 3.5, move -0.5\n",
    "    (get_robot_state(3.0), np.array([-0.5])),  # At dist 3.0, move -0.5\n",
    "    (get_robot_state(2.5), np.array([-0.5])),  # At dist 2.5, move -0.5\n",
    "    (get_robot_state(2.0), np.array([-0.4])),  # At dist 2.0, move -0.4\n",
    "    (get_robot_state(1.6), np.array([-0.3])),  # At dist 1.6, move -0.3\n",
    "    (get_robot_state(1.3), np.array([-0.2])),  # At dist 1.3, move -0.2 \n",
    "    (get_robot_state(1.0), np.array([-0.15])), # New fine step 1\n",
    "    (get_robot_state(0.8), np.array([-0.1])),  # New fine step 2\n",
    "    (get_robot_state(0.5), np.array([-0.05])), # New fine step 3 (Final positioning)\n",
    "]\n",
    "\n",
    "print(f\"Expert Dataset size: {len(expert_trajectory)} observations.\")\n",
    "print(\"Example observation (State, Action):\", expert_trajectory[0])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# FILL IN\n",
    "custom_expert_trajectory = []\n",
    "\n",
    "# UNCOMMENT LINES BELOW TO PRINT STATISTICS ABOUT CUSTOM_EXPERT_TRAJECTORY DATASET\n",
    "# print(f\"Expert Dataset size: {len(custom_expert_trajectory)} observations.\")\n",
    "# print(\"Example observation (State, Action):\", custom_expert_trajectory[0])\n",
    "# print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c7da2d",
   "metadata": {},
   "source": [
    "### (C) Training (Behavioral Cloning Policy)\n",
    "\n",
    "In imitation learning, a **policy** is a function that determines how the robot should behave given any current state by mapping states to actions to create (state, action) pairs. An **optimal policy** produces the best possible strategy for the robot to achieve the desired goal.\n",
    "\n",
    "A **Behavioral Cloning (BC) policy** is a type of imitation learning policy that specifically has the robot learn to mimic the behavor of human experts through copying the experts' behaviors in a supervised learning approach.\n",
    "\n",
    "**Covariate Shift** occurs when the Nearest Neighbor found in the expert trajectory dataset is too far away from the actual current state of the robot. In this case, using the action associated with the nearest expert trajectory is not expected to perform well (or bring the robot closer to the target) with the current state because the policy has to extrapolate too far from the training data.\n",
    "\n",
    "Run the following cell to define the given BC policy with covariate shift logic and train the policy with the expert trajectories.\n",
    "1. Why is Nearest Neighbor an appropriate logic choice for the training policy in imitation learning?\n",
    "2. How could an expert trajectory dataset be improved to minimize the chance of having covariate shift while training the policy? Are there trade offs with this and optimizing for a simple IL setup?\n",
    "3. Based on the output of the cell, was there covariate shift during training with the `expert_trajectory` dataset?\n",
    "4. Add code at the bottom of the cell to train the same policy with the `custom_expert_trajectory` dataset. Did this training experience covariate shift?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f535a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Training (Behavioral Cloning Policy) ---\n",
    "\n",
    "def train_policy_bc(dataset):\n",
    "    \"\"\"\n",
    "    Simulates training a Behavioral Cloning (BC) policy using Nearest Neighbor.\n",
    "    Crucially, it simulates catastrophic failure (extrapolation) if the state\n",
    "    is too far from the training data (Covariate Shift).\n",
    "    \"\"\"\n",
    "    states = [item[0][0] for item in dataset]\n",
    "    actions = [item[1] for item in dataset]\n",
    "\n",
    "    def policy(current_state):\n",
    "        current_distance = current_state[0]\n",
    "        min_distance = float('inf')\n",
    "        best_action = np.array([0.0])\n",
    "\n",
    "        # Nearest Neighbor\n",
    "        for i, expert_dist in enumerate(states):\n",
    "            # Find the state in the dataset that is closest to the current state\n",
    "            diff = abs(expert_dist - current_distance)\n",
    "            if diff < min_distance:\n",
    "                min_distance = diff\n",
    "                best_action = actions[i]\n",
    "        \n",
    "        # --- COVARIATE SHIFT SIMULATION LOGIC ---\n",
    "        # If the nearest training example is more than 1.0 unit away, \n",
    "        # the policy is \"extrapolating\" too far, and it simulates a catastrophic, wrong action.\n",
    "        if min_distance > 1.0:\n",
    "            print(f\"!!! COVARIATE SHIFT WARNING: State {current_distance:.2f} is too far from expert data. Extrapolating to a BAD action.\")\n",
    "            # Wildly incorrect extrapolation, moving away from the target\n",
    "            return np.array([5.0]) \n",
    "\n",
    "        # Introduce a small amount of \"noise\" that all real robots have\n",
    "        # NOISE REDUCED for better convergence near the target\n",
    "        noise = random.uniform(-0.02, 0.02)\n",
    "        best_action[0] += noise \n",
    "        \n",
    "        return best_action\n",
    "\n",
    "    return policy\n",
    "\n",
    "# Train the policy using the expert data\n",
    "trained_policy = train_policy_bc(expert_trajectory)\n",
    "print(\"--- Step 2: Policy Training (Behavioral Cloning) ---\")\n",
    "# Range updated based on new expert data\n",
    "print(\"Policy trained on the expert's demonstrations. It works well only in the range [0.5, 10.0].\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# ADD CODE HERE FOR CUSTOM_EXPERT_TRAJECTORY DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2103b5bf",
   "metadata": {},
   "source": [
    "### (D) Testing & Covariate Shift Demonstration\n",
    "\n",
    "The next step after training the policy on the expert trajectories is to test the policy on unseen trajectories to observe its ability to robustly reach the target.\n",
    "\n",
    "Run the following cell to test the policy trained with the `expert_trajectory` dataset with at most 25 steps.\n",
    "1. In the example below, we allow the robot to take 25 steps towards the target before deciding that it was unable to reach the goal. What are two factors to think about when choosing a max number of steps to allow the system to take to reach the target before determining task failure?\n",
    "2. We discuss safety more in-depth in the Lab Part 2, but the concept is introduced in the `test_policy` function. What part of the function is implementing a safety measure? Why is this safety measure important for our task?\n",
    "3. Two tests are performed below, one with successful completion of the task and one demonstrating the case of failure from covariate shift. Describe what happened to the robot in both scenarios, walking through it's behavior from the initial state until completion/failure.\n",
    "\n",
    ">**Note:** You may need to _View as a scrollable element or open in a text editor_ to see the full output.\n",
    "\n",
    "4. Add code to the bottom of the cell to test the policy trained on the `custom_expert_trajectory` dataset with two different initial positions. How does this policy perform? Does it succeed or fail for your chosen initial positions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Testing & Covariate Shift Demonstration ---\n",
    "\n",
    "def test_policy(policy, initial_position, max_steps=25): # Max steps remains 25\n",
    "    \"\"\"Executes the task and records the robot's performance.\"\"\"\n",
    "    print(f\"\\n[TEST START] Initial Position (Distance): {initial_position:.2f}\")\n",
    "    current_position = initial_position\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        current_state = get_robot_state(current_position)\n",
    "        \n",
    "        # Check for task completion\n",
    "        if abs(current_state[0]) < 0.1:\n",
    "            print(f\"--> [SUCCESS] Task complete in {step} steps. Final position: {current_position:.2f}\")\n",
    "            return True, current_position\n",
    "\n",
    "        # Policy selects the action\n",
    "        action = policy(current_state)\n",
    "        \n",
    "        # Execute the action (with a slight chance of real-world drift)\n",
    "        current_position = execute_action(current_position, action)\n",
    "        \n",
    "        print(f\"Step {step+1}: State (Dist)={current_state[0]:.2f} -> Action={action[0]:.2f} -> New Pos={current_position:.2f}\")\n",
    "\n",
    "        # Check for catastrophic failure (e.g., movement outside sensible bounds)\n",
    "        if current_position < -2.0 or current_position > 20.0:\n",
    "            print(f\"*** [FAILURE] Robot crashed/overshot after reaching an unseen state. Policy unable to recover. ***\")\n",
    "            return False, current_position\n",
    "            \n",
    "    print(f\"--- [FAILURE] Max steps reached. Task incomplete. Final position: {current_position:.2f}\")\n",
    "    return False, current_position\n",
    "\n",
    "print(\"--- Step 3: Execution (Testing the Robot) ---\")\n",
    "print(\"--- Test 3A: In-Distribution (Easy Test) ---\")\n",
    "# Start at a position very close to one in the training data (e.g., 9.8)\n",
    "test_policy(trained_policy, initial_position=9.8)\n",
    "\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "print(\"--- Test 3B: Out-of-Distribution (The Covariate Shift Test) ---\")\n",
    "# Start at a position far outside the expert's initial range (e.g., 11.5).\n",
    "# This is a state the policy has never seen, triggering the failure mechanism.\n",
    "success, failure_pos = test_policy(trained_policy, initial_position=11.5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "## ADD CODE TO TEST POLICY TRAINED ON CUSTOM_EXPERT_TRAJECTORY DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681bc392",
   "metadata": {},
   "source": [
    "### (E) Interactive Correction and Retraining\n",
    "\n",
    "An important step in imitation learning is correction and retraining of the initial policy. If the robot fails to reach the target, we can have the expert intervene to provide a correct action for the state where covariate shift caused a BAD next action to be taken, add this new (state, action) pair to the `expert_trajectory` dataset, and retrain and execute the policy.\n",
    "\n",
    "Run the following cell to \"intervene\" with an expert correction, retrain the BC policy, and execute the policy again for the initial state that failed in part (D).\n",
    "1. Was the robot successful with the retrained policy?\n",
    "2. Test the intervention & retraining with a different expert `corrective_action`. Was the robot successful with the retrained policy?\n",
    "\n",
    ">**Note:** You may need to _View as a scrollable element or open in a text editor_ to see the full output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c991cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Interactive Correction and Retraining ---\n",
    "\n",
    "if not success:\n",
    "    print(\"--- Step 4: Expert Intervention & Retraining ---\")\n",
    "    print(\"The robot failed due to Covariate Shift! The crucial error was made at the initial out-of-distribution state.\")\n",
    "    print(\"The human Expert must now intervene and teach the correct action for that failure state.\")\n",
    "    \n",
    "    # Target the initial out-of-distribution state (11.50)\n",
    "    corrective_position = 11.50 \n",
    "    failure_state = get_robot_state(corrective_position)\n",
    "    \n",
    "    # The expert decides a large, safe step back is needed to get back into the known range (near 8.0)\n",
    "    corrective_action = np.array([-3.5]) \n",
    "\n",
    "    print(f\"EXPERT ACTION: State (Dist)={failure_state[0]:.2f} -> Corrective Action={corrective_action[0]:.2f}\")\n",
    "    \n",
    "    # Add the new, corrected observation to the dataset\n",
    "    expert_trajectory.append((failure_state, corrective_action))\n",
    "    \n",
    "    # Retrain the policy with the new data\n",
    "    retrained_policy = train_policy_bc(expert_trajectory)\n",
    "    print(\"\\nPolicy retrained with one crucial corrective demonstration for the boundary state (11.50).\")\n",
    "\n",
    "    print(\"\\n--- Test 4: Re-Execution with Retrained Policy ---\")\n",
    "    # Test again from the problematic starting position (11.5)\n",
    "    success_retrained, _ = test_policy(retrained_policy, initial_position=11.5)\n",
    "\n",
    "    if success_retrained:\n",
    "        print(\"\\n[CONCLUSION]: The robot SUCCEEDED! By correcting the boundary state, the policy knew how to recover and re-entered the known distribution.\")\n",
    "    else:\n",
    "        print(\"\\n[CONCLUSION]: The robot FAILED even after correction, indicating more demonstrations might be needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744cd87",
   "metadata": {},
   "source": [
    ">**Congratulations! You have finished section 1 of the lab.** Continue to section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2fb20e",
   "metadata": {},
   "source": [
    "### Continue to Lab Part 2\n",
    "0. [Lab Part 2 README.md](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/README.md)\n",
    "1. [1_chatgpt.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/1_chatgpt.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2095ae",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [What is Imitation Learning? - NVIDIA](https://www.nvidia.com/en-us/glossary/imitation-learning/j8)\n",
    "* [A brief overview of Imitation Learning](https://smartlabai.medium.com/a-brief-overview-of-imitation-learning-8a8a75c44a9c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
