{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbc371",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# This script simulates an interactive Imitation Learning (IL) activity for a simple robotics \"Pick-and-Place\" task using Behavioral Cloning.\n",
    "#\n",
    "# The goal is to demonstrate:\n",
    "# 1. Expert Demonstration (Data Collection)\n",
    "# 2. Policy Training (Supervised Learning)\n",
    "# 3. Policy Execution and Failure due to Covariate Shift (Testing)\n",
    "# 4. Interactive Correction and Retraining\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# --- 1. Environment and Robot Setup ---\n",
    "\n",
    "# Robot State (Observation) representation:\n",
    "# We simplify the task into a single dimension: distance to target.\n",
    "# State S = [distance_to_target]\n",
    "# Action A = [movement_step]\n",
    "\n",
    "TARGET_DISTANCE = 0.0 # The task is complete when distance is close to zero.\n",
    "\n",
    "def get_robot_state(current_position):\n",
    "    \"\"\"Simulates getting the robot's current state (distance to target).\"\"\"\n",
    "    return np.array([current_position - TARGET_DISTANCE])\n",
    "\n",
    "def execute_action(current_position, action):\n",
    "    \"\"\"Simulates the robot moving based on the action.\"\"\"\n",
    "    # The new position is the current position adjusted by the action (movement step)\n",
    "    new_position = current_position + action[0]\n",
    "    return new_position\n",
    "\n",
    "\n",
    "# --- 4. Testing & Covariate Shift Demonstration ---\n",
    "\n",
    "def test_policy(policy, initial_position, max_steps=25): # Max steps remains 25\n",
    "    \"\"\"Executes the task and records the robot's performance.\"\"\"\n",
    "    print(f\"\\n[TEST START] Initial Position (Distance): {initial_position:.2f}\")\n",
    "    current_position = initial_position\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        current_state = get_robot_state(current_position)\n",
    "        \n",
    "        # Check for task completion\n",
    "        if abs(current_state[0]) < 0.1:\n",
    "            print(f\"--> [SUCCESS] Task complete in {step} steps. Final position: {current_position:.2f}\")\n",
    "            return True, current_position\n",
    "\n",
    "        # Policy selects the action\n",
    "        action = policy(current_state)\n",
    "        \n",
    "        # Execute the action (with a slight chance of real-world drift)\n",
    "        current_position = execute_action(current_position, action)\n",
    "        \n",
    "        print(f\"Step {step+1}: State (Dist)={current_state[0]:.2f} -> Action={action[0]:.2f} -> New Pos={current_position:.2f}\")\n",
    "\n",
    "        # Check for catastrophic failure (e.g., movement outside sensible bounds)\n",
    "        if current_position < -2.0 or current_position > 20.0:\n",
    "            print(f\"*** [FAILURE] Robot crashed/overshot after reaching an unseen state. Policy unable to recover. ***\")\n",
    "            return False, current_position\n",
    "            \n",
    "    print(f\"--- [FAILURE] Max steps reached. Task incomplete. Final position: {current_position:.2f}\")\n",
    "    return False, current_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29243e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 2. Demonstration (Data Collection - The Expert Phase) ---\n",
    "\n",
    "# The Expert (the user) provides demonstrations for a successful run.\n",
    "print(\"--- Step 1: Expert Demonstration (Data Collection) ---\")\n",
    "print(\"The Expert (user) provides successful movement trajectories to reach the target (10.0 -> 0.0).\")\n",
    "# \n",
    "\n",
    "# The Expert dataset D = [(State, Action), ...]\n",
    "# Adjusted with more fine-grained steps near the end for better convergence\n",
    "expert_trajectory = [\n",
    "    (get_robot_state(10.0), np.array([-2.0])), # At dist 10, move -2.0\n",
    "    (get_robot_state(8.0), np.array([-1.5])),  # At dist 8, move -1.5\n",
    "    (get_robot_state(6.5), np.array([-1.0])),  # At dist 6.5, move -1.0\n",
    "    (get_robot_state(5.5), np.array([-1.0])),  # At dist 5.5, move -1.0\n",
    "    (get_robot_state(4.5), np.array([-0.5])),  # At dist 4.5, move -0.5\n",
    "    (get_robot_state(4.0), np.array([-0.5])),  # At dist 4.0, move -0.5\n",
    "    (get_robot_state(3.5), np.array([-0.5])),  # At dist 3.5, move -0.5\n",
    "    (get_robot_state(3.0), np.array([-0.5])),  # At dist 3.0, move -0.5\n",
    "    (get_robot_state(2.5), np.array([-0.5])),  # At dist 2.5, move -0.5\n",
    "    (get_robot_state(2.0), np.array([-0.4])),  # At dist 2.0, move -0.4\n",
    "    (get_robot_state(1.6), np.array([-0.3])),  # At dist 1.6, move -0.3\n",
    "    (get_robot_state(1.3), np.array([-0.2])),  # At dist 1.3, move -0.2 \n",
    "    (get_robot_state(1.0), np.array([-0.15])), # New fine step 1\n",
    "    (get_robot_state(0.8), np.array([-0.1])),  # New fine step 2\n",
    "    (get_robot_state(0.5), np.array([-0.05])), # New fine step 3 (Final positioning)\n",
    "]\n",
    "\n",
    "print(f\"Expert Dataset size: {len(expert_trajectory)} observations.\")\n",
    "print(\"Example observation (State, Action):\", expert_trajectory[0])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f535a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. Training (Behavioral Cloning Policy) ---\n",
    "\n",
    "def train_policy_bc(dataset):\n",
    "    \"\"\"\n",
    "    Simulates training a Behavioral Cloning (BC) policy using Nearest Neighbor.\n",
    "    Crucially, it simulates catastrophic failure (extrapolation) if the state\n",
    "    is too far from the training data (Covariate Shift).\n",
    "    \"\"\"\n",
    "    states = [item[0][0] for item in dataset]\n",
    "    actions = [item[1] for item in dataset]\n",
    "\n",
    "    def policy(current_state):\n",
    "        current_distance = current_state[0]\n",
    "        min_distance = float('inf')\n",
    "        best_action = np.array([0.0])\n",
    "\n",
    "        for i, expert_dist in enumerate(states):\n",
    "            # Find the state in the dataset that is closest to the current state\n",
    "            diff = abs(expert_dist - current_distance)\n",
    "            if diff < min_distance:\n",
    "                min_distance = diff\n",
    "                best_action = actions[i]\n",
    "        \n",
    "        # --- COVARIATE SHIFT SIMULATION LOGIC ---\n",
    "        # If the nearest training example is more than 1.0 unit away, \n",
    "        # the policy is \"extrapolating\" too far, and it simulates a catastrophic, wrong action.\n",
    "        if min_distance > 1.0:\n",
    "            print(f\"!!! COVARIATE SHIFT WARNING: State {current_distance:.2f} is too far from expert data. Extrapolating to a BAD action.\")\n",
    "            # Wildly incorrect extrapolation, moving away from the target\n",
    "            return np.array([5.0]) \n",
    "\n",
    "        # Introduce a small amount of \"noise\" that all real robots have\n",
    "        # NOISE REDUCED for better convergence near the target\n",
    "        noise = random.uniform(-0.02, 0.02)\n",
    "        best_action[0] += noise \n",
    "        \n",
    "        return best_action\n",
    "\n",
    "    return policy\n",
    "\n",
    "# Train the policy using the expert data\n",
    "trained_policy = train_policy_bc(expert_trajectory)\n",
    "print(\"--- Step 2: Policy Training (Behavioral Cloning) ---\")\n",
    "# Range updated based on new expert data\n",
    "print(\"Policy trained on the expert's demonstrations. It works well only in the range [0.5, 10.0].\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69e0b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"--- Step 3: Execution (Testing the Robot) ---\")\n",
    "print(\"--- Test 3A: In-Distribution (Easy Test) ---\")\n",
    "# Start at a position very close to one in the training data (e.g., 9.8)\n",
    "test_policy(trained_policy, initial_position=9.8)\n",
    "\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "print(\"--- Test 3B: Out-of-Distribution (The Covariate Shift Test) ---\")\n",
    "# Start at a position far outside the expert's initial range (e.g., 11.5).\n",
    "# This is a state the policy has never seen, triggering the failure mechanism.\n",
    "success, failure_pos = test_policy(trained_policy, initial_position=11.5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c991cc1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 5. Interactive Correction and Retraining ---\n",
    "\n",
    "if not success:\n",
    "    print(\"--- Step 4: Expert Intervention & Retraining ---\")\n",
    "    print(\"The robot failed due to Covariate Shift! The crucial error was made at the initial out-of-distribution state.\")\n",
    "    print(\"The human Expert must now intervene and teach the correct action for that failure state.\")\n",
    "    \n",
    "    # Target the initial out-of-distribution state (11.50)\n",
    "    corrective_position = 11.50 \n",
    "    failure_state = get_robot_state(corrective_position)\n",
    "    \n",
    "    # The expert decides a large, safe step back is needed to get back into the known range (near 8.0)\n",
    "    corrective_action = np.array([-3.5]) \n",
    "\n",
    "    print(f\"EXPERT ACTION: State (Dist)={failure_state[0]:.2f} -> Corrective Action={corrective_action[0]:.2f}\")\n",
    "    \n",
    "    # Add the new, corrected observation to the dataset\n",
    "    expert_trajectory.append((failure_state, corrective_action))\n",
    "    \n",
    "    # Retrain the policy with the new data\n",
    "    retrained_policy = train_policy_bc(expert_trajectory)\n",
    "    print(\"\\nPolicy retrained with one crucial corrective demonstration for the boundary state (11.50).\")\n",
    "\n",
    "    print(\"\\n--- Test 4: Re-Execution with Retrained Policy ---\")\n",
    "    # Test again from the problematic starting position (11.5)\n",
    "    success_retrained, _ = test_policy(retrained_policy, initial_position=11.5)\n",
    "\n",
    "    if success_retrained:\n",
    "        print(\"\\n[CONCLUSION]: The robot SUCCEEDED! By correcting the boundary state, the policy knew how to recover and re-entered the known distribution.\")\n",
    "    else:\n",
    "        print(\"\\n[CONCLUSION]: The robot FAILED even after correction, indicating more demonstrations might be needed.\")\n",
    "\n",
    "# Final check to ensure the file ends with the required marker"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
