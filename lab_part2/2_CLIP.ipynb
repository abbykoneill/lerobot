{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f82a1e",
   "metadata": {},
   "source": [
    "# Introduction to CLIP\n",
    "\n",
    "### Lab Table of Contents\n",
    "1. [1_chatgpt.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/1_chatgpt.ipynb)\n",
    "2. **[2_CLIP.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/2_CLIP.ipynb)**\n",
    "3. [3_VLM_BLIP.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/3_VLM_BLIP.ipynb)\n",
    "4. [4_VLA.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/4_VLA.ipynb)\n",
    "5. [5_safety.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/5_safety.ipynb)\n",
    "6. [Lab Checkoff](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/checkoff.txt)\n",
    "\n",
    "## CLIP (Contrastive Languageâ€“Image Pre-training)\n",
    "CLIP is a neural network trained on (image, text) pairs that uses natural language processing to product text when given an input image. Pre-training on 400M (image, text) pairs, CLIP learns about images from raw text and then uses an image encoder and text encoder to focus on visual and text features, respectively. This model was initially designed to address several problems with deep learning approaches for computer vision.\n",
    "\n",
    "In this lab, you will use the [HuggingFace implementation of CLIP](https://huggingface.co/docs/transformers/model_doc/clip). Walk through the two CLIP examples below.\n",
    "\n",
    "Make sure to save your outputs and discuss answers to the questions with your lab partner.\n",
    "\n",
    "#### Before Beginning with Code - Complete Environment Set-Up:\n",
    "* `conda create -n <env_name> python=3.10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ec379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "\n",
    "!pip install ipykernel\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ad9d4",
   "metadata": {},
   "source": [
    "## CLIP Vision Model with Projection\n",
    "\n",
    "This model projects visual features from an input image into the same latent space as text features in order to compute a similarity score, enabling zero-shot image classification and text-based image retrieval.\n",
    "\n",
    "1. Run the vision model code below. Save and comment on the output. How does this differ from using an image as an input for ChatGPT in notebook 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1afe9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Vision Model with Projection\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, CLIPVisionModelWithProjection\n",
    "from transformers.image_utils import load_image\n",
    "\n",
    "model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = load_image(url)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model(**inputs)\n",
    "image_embeds = outputs.image_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce472181",
   "metadata": {},
   "source": [
    "## CLIP Image Classification\n",
    "\n",
    "This model performs zero-shot image classification, classifying images into categories without prior knowledge or explicit training on those categories. This is enabled through CLIP's dense pre-training on (image, text) pairs.\n",
    "\n",
    "2. Run the image classification model code below. Save and comment on the output. How does this differ from using an image as an input for ChatGPT in notebook 1? How does it differ from the vision model with projection above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Classification with CLIP\n",
    "\n",
    "from transformers import AutoImageProcessor, CLIPForImageClassification\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = CLIPForImageClassification.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "print(model.config.id2label[predicted_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eeab61",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [HuggingFace CLIP](https://huggingface.co/docs/transformers/model_doc/clip)\n",
    "* [OpenAI CLIP GitHub Repository](https://github.com/openai/CLIP)\n",
    "* [OpenAI CLIP Blog](https://openai.com/index/clip/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
