{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f82a1e",
   "metadata": {},
   "source": [
    "# Introduction to CLIP\n",
    "\n",
    "### Lab Table of Contents\n",
    "* Part 1\n",
    "    1. [1_imitation_learning.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part_1/1_imitation_learning.ipynb)\n",
    "* **Part 2**\n",
    "    1. [1_chatgpt.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/1_chatgpt.ipynb)\n",
    "    2. **[2_CLIP.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/2_CLIP.ipynb)**\n",
    "    3. [3_VLM_BLIP.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/3_VLM_BLIP.ipynb)\n",
    "    4. [4_VLA.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/4_VLA.ipynb)\n",
    "    5. [5_safety.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/5_safety.ipynb)\n",
    "* [Lab Checkoff](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/checkoff.md)\n",
    "\n",
    "## CLIP (Contrastive Languageâ€“Image Pre-training)\n",
    "CLIP is a neural network trained on (image, text) pairs that uses natural language processing to produce text when given an input image. Pre-training on 400M (image, text) pairs, CLIP learns about images from raw text and then uses an image encoder and text encoder to focus on visual and text features, respectively. This model was initially designed to address several problems with deep learning approaches for computer vision.\n",
    "\n",
    "In this lab, you will use the [HuggingFace implementation of CLIP](https://huggingface.co/docs/transformers/model_doc/clip). Walk through the two CLIP examples below.\n",
    "\n",
    "Make sure to save your outputs and discuss answers to the questions with your lab partner.\n",
    "\n",
    "#### Before Beginning with Code - Complete Environment Set-Up:\n",
    "* `conda create -n <env_name> python=3.10`\n",
    "\n",
    ">**Note:** The CLIP model requires a good amount of space. Before beginning with this notebook, it might be helpful to either free up local space or use Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ec379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "\n",
    "!pip install ipykernel\n",
    "!pip install torch==2.9.1\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce472181",
   "metadata": {},
   "source": [
    "## CLIP Image Classification\n",
    "\n",
    "This model performs zero-shot image classification, classifying images into categories without prior knowledge or explicit training on those categories. This is enabled through CLIP's dense pre-training on (image, text) pairs.\n",
    "\n",
    "The three subsequent code cells use three different variations of the CLIP model for image classification on the same image.\n",
    "1. Run the three cells to test the three different models. Save the output image description from each model.\n",
    "2. How accurate is each of the descriptions? Why do you think some perform better than others?\n",
    "3. The `labels` list in the third model is a list of predicted image labels to use for classifying the image. Change some of the values and rerun this third model. Experiment with labels that are very specific to the image, ones that are very incorrect for the image, and what happens if you have two labels that are similar to each other. \n",
    "2. How does CLIP Image Classification differ from using an image as an input for ChatGPT in notebook 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Classification with CLIP\n",
    "\n",
    "# Image to Classify\n",
    "\n",
    "from transformers import AutoImageProcessor, CLIPForImageClassification\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "import json\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title('Input Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10afc9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Classification with CLIP\n",
    "\n",
    "# CLIP Model Variation 1\n",
    "\n",
    "modelname = \"openai/clip-vit-base-patch32\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(modelname)\n",
    "model = CLIPForImageClassification.from_pretrained(modelname)\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "#print(model.config.id2label[predicted_label])\n",
    "\n",
    "# Download imagenet class names\n",
    "url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "class_names = urllib.request.urlopen(url).read().decode(\"utf-8\").split(\"\\n\")\n",
    "\n",
    "pred_label = class_names[predicted_label]\n",
    "print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96822140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Classification with CLIP\n",
    "\n",
    "# CLIP Model Variation 2\n",
    "\n",
    "modelname = \"google/vit-base-patch16-224\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(modelname)\n",
    "model = CLIPForImageClassification.from_pretrained(modelname)\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "print(model.config.id2label[predicted_label])\n",
    "\n",
    "# Download imagenet class names\n",
    "url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "class_names = urllib.request.urlopen(url).read().decode(\"utf-8\").split(\"\\n\")\n",
    "\n",
    "pred_label = class_names[predicted_label]\n",
    "print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Classification with CLIP\n",
    "\n",
    "# CLIP Model Variation 3\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# update this list with different predicted labels\n",
    "labels = [\"tabby cat\", \"golden retriever\", \"goldfish\", \"email\", \"black cat\"]\n",
    "inputs = processor(text=labels, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits_per_image\n",
    "\n",
    "pred = logits.argmax().item()\n",
    "print(labels[pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e80e7",
   "metadata": {},
   "source": [
    "### Continue to\n",
    "[3_VLM_BLIP.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/3_VLM_BLIP.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eeab61",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [HuggingFace CLIP](https://huggingface.co/docs/transformers/model_doc/clip)\n",
    "* [OpenAI CLIP GitHub Repository](https://github.com/openai/CLIP)\n",
    "* [OpenAI CLIP Blog](https://openai.com/index/clip/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
