{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4edcef",
   "metadata": {},
   "source": [
    "# Introduction and Deployment of VLA: SMolVLA\n",
    "\n",
    "## VLA: Vision-Language Action Model\n",
    "Compared to VLMs, a VLA is specifically made for robotics where you want to be able to use language to help understand the correct action a robot should take.\n",
    "\n",
    "SmolVLA is an example of a VLA foundation model where input the model are \n",
    "\n",
    "For a more comprehensive overview on all LeRobot has to offer, [LeRobot Repo](https://learnopencv.com/vision-language-action-models-lerobot-policy/). \n",
    "\n",
    "Walk through each cell of the notebook to better understand the VLA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a070fbef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ipykernel matplotlib\n",
    "!pip install lerobot\n",
    "!conda install ffmpeg -c conda-forge -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3840c1",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a617a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "from lerobot.common.policies.lerobot_policy import LeRobotPolicy\n",
    "\n",
    "def show_image(img, instruction):\n",
    "    \"\"\"Display an image with its instruction.\"\"\"\n",
    "    plt.imshow(img.permute(1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n",
    "    plt.title(f\"Instruction: \\\"{instruction}\\\"\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1d5d7f",
   "metadata": {},
   "source": [
    "### Load the Pre-Trained VLA (SmolVLA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb68de8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the pre-trained SmolVLA policy\n",
    "# This downloads the model weights from the Hugging Face Hub\n",
    "policy = LeRobotPolicy.from_pretrained(\"lerobot/smolvla_base\").to(device)\n",
    "\n",
    "# Set the model to evaluation mode (disables things like dropout)\n",
    "policy.eval()\n",
    "\n",
    "print(\"SmolVLA model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c3979",
   "metadata": {},
   "source": [
    "### Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeff57a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset_repo_id = \"lerobot/svla_so100_pickplace\"\n",
    "dataset = LeRobotDataset(dataset_repo_id)\n",
    "\n",
    "print(f\"Loaded dataset with {len(dataset)} total steps.\")\n",
    "\n",
    "# Let's get a single sample from the dataset (e.g., sample #1000)\n",
    "sample_idx = 1000\n",
    "sample = dataset[sample_idx]\n",
    "\n",
    "# Let's see what's in our sample\n",
    "print(\"\\nSample keys:\")\n",
    "print(sample.keys())\n",
    "\n",
    "# The 'observation' key contains the inputs for the model\n",
    "print(\"\\nObservation keys:\")\n",
    "print(sample['observation'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aafe8d",
   "metadata": {},
   "source": [
    "### Prepare Inputs and Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1954f01",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get the instruction and observation from our sample\n",
    "instruction = sample['instruction']\n",
    "\n",
    "# Prepare the observation dictionary\n",
    "# We add a batch dimension (B) to each tensor, e.g., (C, H, W) -> (B, C, H, W)\n",
    "observation = {\n",
    "    'images.top': sample['observation']['images.top'].to(device).unsqueeze(0),\n",
    "    'state': sample['observation']['state'].to(device).unsqueeze(0)\n",
    "}\n",
    "\n",
    "# Run inference!\n",
    "# We use torch.no_grad() to tell PyTorch we're not training, which saves memory.\n",
    "with torch.no_grad():\n",
    "    action = policy.select_action(observation, instruction)\n",
    "\n",
    "print(f\"Instruction: \\\"{instruction}\\\"\")\n",
    "print(f\"\\nPredicted Action: {action.cpu().numpy()}\")\n",
    "\n",
    "# Let's also see what the \"ground truth\" action was (what the human did)\n",
    "ground_truth_action = sample['action'].numpy()\n",
    "print(f\"Ground Truth Action: {ground_truth_action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e4bc6",
   "metadata": {},
   "source": [
    "### Visualize the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d71aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "top_image = observation['images.top'][0].cpu()\n",
    "\n",
    "# Use our helper function to show the image and instruction\n",
    "show_image(top_image, instruction)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
