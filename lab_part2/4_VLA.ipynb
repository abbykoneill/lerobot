{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding Vision-Language-Action Models (VLAs)\n",
        "\n",
        "### Lab Table of Contents\n",
        "* Part 1\n",
        "    1. [1_imitation_learning.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part_1/1_imitation_learning.ipynb)\n",
        "* **Part 2**\n",
        "    1. [1_chatgpt.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/1_chatgpt.ipynb)\n",
        "    2. [2_CLIP.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/2_CLIP.ipynb)\n",
        "    3. [3_VLM_BLIP.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/3_VLM_BLIP.ipynb)\n",
        "    4. **[4_VLA.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/4_VLA.ipynb)**\n",
        "    5. [5_safety.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/5_safety.ipynb)\n",
        "* [Lab Checkoff](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/checkoff.md)\n",
        "\n",
        "### What is a VLA?\n",
        "\n",
        "A **Vision-Language-Action (VLA)** model is the bridge between perception, language understanding, and robot control.\n",
        "\n",
        "```\n",
        "┌─────────────┐     ┌─────────────┐     ┌─────────────┐\n",
        "│   Camera    │     │  Instruction│     │ Robot State │\n",
        "│   Image     │     │   \"pick up  │     │  (joints,   │\n",
        "│             │     │   the red   │     │   gripper)  │\n",
        "│             │     │   block\"    │     │             │\n",
        "└──────┬──────┘     └──────┬──────┘     └──────┬──────┘\n",
        "       │                   │                   │\n",
        "       └───────────────────┼───────────────────┘\n",
        "                           │\n",
        "                    ┌──────▼──────┐\n",
        "                    │     VLA     │\n",
        "                    │    Model    │\n",
        "                    └──────┬──────┘\n",
        "                           │\n",
        "                    ┌──────▼──────┐\n",
        "                    │   Action    │\n",
        "                    │  Commands   │\n",
        "                    │ (move arm,  │\n",
        "                    │  close grip)│\n",
        "                    └─────────────┘\n",
        "```\n",
        "\n",
        "### Key Difference from VLMs\n",
        "\n",
        "| VLM (Vision-Language Model) | VLA (Vision-Language-Action Model) |\n",
        "|-----------------------------|------------------------------------|  \n",
        "| Input: Image + Text | Input: Image + Text + Robot State |\n",
        "| Output: **Text** | Output: **Actions** |\n",
        "| Example: \"This is a cat\" | Example: `[0.1, -0.3, 0.5, 0.2, 0.0, 0.1, 1.0]` |\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "1. Understand the architecture of a VLA model\n",
        "2. Build a simplified VLA from scratch\n",
        "3. Understand how vision, language, and state are fused\n",
        "4. Learn about action tokens vs. continuous actions\n",
        "5. Experiment with different model configurations\n",
        "\n",
        "#### Ensure you have a new environment Set-Up:\n",
        "* `conda create -n <env_name> python=3.10`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Install dependencies and import educational VLA module\n",
        "\n",
        "!pip install torch numpy matplotlib --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import our VLA module\n",
        "from VLA_exercises import (\n",
        "    SimpleVLA, \n",
        "    VisionEncoder, \n",
        "    LanguageEncoder, \n",
        "    StateEncoder,\n",
        "    MultimodalFusion,\n",
        "    ActionDecoder,\n",
        "    SimpleTokenizer,\n",
        "    create_demo_inputs,\n",
        "    run_demo\n",
        ")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"✓ All modules loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the quick demo below to see what a VLA does!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the demo from our module\n",
        "model, output = run_demo()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Each Component\n",
        "\n",
        "A VLA has three main input encoders. Let's examine each one.\n",
        "\n",
        "### 1. Vision Encoder\n",
        "\n",
        "The vision encoder converts camera images into a feature vector that captures the visual scene.\n",
        "\n",
        "**Real VLAs use:** ViT (Vision Transformer), SigLIP, CLIP visual encoder  \n",
        "**Our educational version uses:** A simple CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vision_encoder = VisionEncoder(input_channels=3, embedding_dim=256)\n",
        "\n",
        "sample_image = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "print(f\"Input image shape: {sample_image.shape}\")\n",
        "print(f\"  - Batch size: {sample_image.shape[0]}\")\n",
        "print(f\"  - Channels (RGB): {sample_image.shape[1]}\")\n",
        "print(f\"  - Height: {sample_image.shape[2]}\")\n",
        "print(f\"  - Width: {sample_image.shape[3]}\")\n",
        "\n",
        "# Encode the image\n",
        "vision_embedding = vision_encoder(sample_image)\n",
        "\n",
        "print(f\"\\nOutput embedding shape: {vision_embedding.shape}\")\n",
        "print(f\"  - The 224x224x3 image is now a 256-dimensional vector!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trace the Dimensions\n",
        "\n",
        "Add print statements to understand how the image dimensions change through the CNN layers.\n",
        "\n",
        "1. Why does the image get smaller as it passes through convolutional layers?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXERCISE 1: Trace dimensions through the vision encoder\n",
        "\n",
        "class VisionEncoderDebug(nn.Module):\n",
        "    \"\"\"Vision encoder with debug print statements.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4, padding=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((4, 4))\n",
        "        self.fc = nn.Linear(128 * 4 * 4, 256)\n",
        "    \n",
        "    def forward(self, image):\n",
        "        print(f\"Input: {image.shape}\")\n",
        "        \n",
        "        x = torch.relu(self.conv1(image))\n",
        "        print(f\"After conv1 (8x8 kernel, stride 4): {x.shape}\")\n",
        "        \n",
        "        # TODO: Fill in the correct shape after conv2\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        print(f\"After conv2: {x.shape}\")\n",
        "        \n",
        "        x = torch.relu(self.conv3(x))\n",
        "        print(f\"After conv3: {x.shape}\")\n",
        "        \n",
        "        x = self.pool(x)\n",
        "        print(f\"After pooling: {x.shape}\")\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        print(f\"After flatten: {x.shape}\")\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        print(f\"Final embedding: {x.shape}\")\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Test your debug encoder\n",
        "debug_encoder = VisionEncoderDebug()\n",
        "_ = debug_encoder(sample_image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Language Encoder\n",
        "\n",
        "The language encoder converts text instructions into a feature vector.\n",
        "\n",
        "**Real VLAs use:** Pretrained LLMs (Llama, SmolLM), CLIP text encoder  \n",
        "**Our educational version uses:** Word embeddings + LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a tokenizer and language encoder\n",
        "tokenizer = SimpleTokenizer(vocab_size=1000)\n",
        "language_encoder = LanguageEncoder(vocab_size=1000, embedding_dim=256)\n",
        "\n",
        "# Sample instructions\n",
        "instructions = [\n",
        "    \"pick up the red block\",\n",
        "    \"move the cup to the left\",\n",
        "    \"push the button\"\n",
        "]\n",
        "\n",
        "print(\"Tokenizing instructions:\\n\")\n",
        "for instruction in instructions:\n",
        "    tokens = tokenizer.encode(instruction)\n",
        "    print(f\"'{instruction}'\")\n",
        "    print(f\"  → Tokens: {tokens[:10].tolist()}... (padded to 32)\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode the first instruction\n",
        "instruction = \"pick up the red block\"\n",
        "tokens = tokenizer.encode(instruction).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "print(f\"Token shape: {tokens.shape}\")\n",
        "\n",
        "language_embedding = language_encoder(tokens)\n",
        "\n",
        "print(f\"Language embedding shape: {language_embedding.shape}\")\n",
        "print(f\"  - The instruction is now a 256-dimensional vector!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. State Encoder\n",
        "\n",
        "The state encoder processes the robot's current configuration (joint positions, gripper state, etc.).\n",
        "\n",
        "For a typical 6-DOF robot arm:\n",
        "- 6 joint angles (shoulder, elbow, wrist, etc.)\n",
        "- 1 gripper state (open/closed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a state encoder\n",
        "state_encoder = StateEncoder(state_dim=7, embedding_dim=256)\n",
        "\n",
        "# Sample robot state: [joint1, joint2, joint3, joint4, joint5, joint6, gripper]\n",
        "robot_state = torch.tensor([[\n",
        "    0.5,   # Joint 1 angle (normalized)\n",
        "    -0.3,  # Joint 2 angle\n",
        "    0.8,   # Joint 3 angle\n",
        "    0.0,   # Joint 4 angle\n",
        "    0.2,   # Joint 5 angle\n",
        "    -0.1,  # Joint 6 angle\n",
        "    1.0    # Gripper (1.0 = closed, -1.0 = open)\n",
        "]])\n",
        "\n",
        "print(f\"Robot state: {robot_state.tolist()[0]}\")\n",
        "print(f\"State shape: {robot_state.shape}\")\n",
        "\n",
        "state_embedding = state_encoder(robot_state)\n",
        "\n",
        "print(f\"\\nState embedding shape: {state_embedding.shape}\")\n",
        "print(f\"  - The 7 joint values are now a 256-dimensional vector!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multimodal Fusion\n",
        "\n",
        "**The key insight of VLAs:** How do we combine vision, language, and state into a unified representation?\n",
        "\n",
        "This is one of the most important design decisions in VLA architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We have three embeddings of the same dimension\n",
        "print(\"Input embeddings:\")\n",
        "print(f\"  Vision:   {vision_embedding.shape}\")\n",
        "print(f\"  Language: {language_embedding.shape}\")\n",
        "print(f\"  State:    {state_embedding.shape}\")\n",
        "\n",
        "# Create fusion modules with different strategies\n",
        "fusion_concat = MultimodalFusion(embedding_dim=256, fusion_dim=512, fusion_type=\"concat\")\n",
        "fusion_attention = MultimodalFusion(embedding_dim=256, fusion_dim=512, fusion_type=\"attention\")\n",
        "\n",
        "# Try concatenation fusion\n",
        "fused_concat = fusion_concat(vision_embedding, language_embedding, state_embedding)\n",
        "print(f\"\\nConcatenation fusion output: {fused_concat.shape}\")\n",
        "\n",
        "# Try attention fusion\n",
        "fused_attention = fusion_attention(vision_embedding, language_embedding, state_embedding)\n",
        "print(f\"Attention fusion output: {fused_attention.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implement Multiplicative Fusion\n",
        "\n",
        "2. Implement a new fusion strategy that multiplies embeddings element-wise.\n",
        "\n",
        "| Fusion Type | Pros | Cons |\n",
        "|-------------|------|------|\n",
        "| Concatenation | Simple, fast | Doesn't capture interactions |\n",
        "| Attention | Learns modality importance | More complex |\n",
        "| Multiplicative | Forces shared representations | FILL IN! |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement multiplicative fusion\n",
        "\n",
        "class MultiplicativeFusion(nn.Module):\n",
        "    \"\"\"Fuses embeddings by element-wise multiplication.\"\"\"\n",
        "    \n",
        "    def __init__(self, embedding_dim=256, fusion_dim=512):\n",
        "        super().__init__()\n",
        "        # TODO: Define the layers needed\n",
        "        # Hint: After multiplying, you need to project to fusion_dim\n",
        "        self.projection = nn.Linear(embedding_dim, fusion_dim)  # Fix this if needed\n",
        "    \n",
        "    def forward(self, vision_emb, language_emb, state_emb):\n",
        "        # TODO: Implement multiplicative fusion\n",
        "        # Step 1: Multiply all three embeddings element-wise\n",
        "        # Step 2: Project to fusion_dim\n",
        "        \n",
        "        # Your code here:\n",
        "        # fused = \n",
        "        # return self.projection(fused)\n",
        "        pass\n",
        "\n",
        "# Test your implementation (uncomment when ready)\n",
        "# mult_fusion = MultiplicativeFusion()\n",
        "# fused_mult = mult_fusion(vision_embedding, language_embedding, state_embedding)\n",
        "# print(f\"Multiplicative fusion output: {fused_mult.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Action Decoding\n",
        "\n",
        "The final stage: converting the fused representation into robot actions.\n",
        "\n",
        "### Two Approaches:\n",
        "\n",
        "1. **Continuous Actions** (Traditional robotics)\n",
        "   - Output: `[-0.3, 0.5, 0.1, ...]` (float values)\n",
        "   - Pro: Precise control\n",
        "   - Con: Hard to use language model techniques\n",
        "\n",
        "2. **Action Tokens** (Modern VLAs like RT-2)\n",
        "   - Output: `[127, 192, 64, ...]` (discrete bins)\n",
        "   - Pro: Can use autoregressive generation like LLMs\n",
        "   - Con: Quantization loses some precision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create action decoders with both approaches\n",
        "decoder_continuous = ActionDecoder(fusion_dim=512, action_dim=7, use_action_tokens=False)\n",
        "decoder_tokens = ActionDecoder(fusion_dim=512, action_dim=7, num_action_bins=256, use_action_tokens=True)\n",
        "\n",
        "print(\"Fused input shape:\", fused_attention.shape)\n",
        "print()\n",
        "\n",
        "# Continuous actions\n",
        "output_continuous = decoder_continuous(fused_attention)\n",
        "print(\"=== Continuous Action Output ===\")\n",
        "print(f\"Actions shape: {output_continuous['actions'].shape}\")\n",
        "print(f\"Actions: {output_continuous['actions'][0].detach().numpy().round(3)}\")\n",
        "print()\n",
        "\n",
        "# Tokenized actions\n",
        "output_tokens = decoder_tokens(fused_attention)\n",
        "print(\"=== Tokenized Action Output ===\")\n",
        "print(f\"Action tokens: {output_tokens['action_tokens'][0].tolist()}\")\n",
        "print(f\"Converted to continuous: {output_tokens['actions'][0].detach().numpy().round(3)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate Action Token Precision\n",
        "\n",
        "If we discretize actions into 256 bins:\n",
        "1. What is the precision (step size) between bins?\n",
        "2. Is this precise enough for delicate robot manipulation?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate action precision\n",
        "\n",
        "num_bins = 256\n",
        "action_range = 2.0  # Actions range from -1 to 1\n",
        "\n",
        "# TODO: Calculate the precision (step size between bins)\n",
        "# Formula: action_range / (num_bins - 1)\n",
        "precision_256 = action_range / (num_bins - 1)\n",
        "\n",
        "print(f\"With {num_bins} bins:\")\n",
        "print(f\"  Action range: [-1, 1] = {action_range}\")\n",
        "print(f\"  Precision: {precision_256:.6f}\")\n",
        "\n",
        "# TODO: What if we only had 16 bins?\n",
        "num_bins_16 = 16\n",
        "precision_16 = action_range / (num_bins_16 - 1)\n",
        "print(f\"\\nWith only {num_bins_16} bins:\")\n",
        "print(f\"  Precision: {precision_16:.6f}\")\n",
        "\n",
        "# Reflection: A typical servo motor has ~0.1 degree precision\n",
        "# Is 256 bins enough?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Complete VLA Pipeline\n",
        "\n",
        "Now let's use the complete SimpleVLA model and see the full pipeline in action.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the complete VLA model\n",
        "vla = SimpleVLA(\n",
        "    image_channels=3,\n",
        "    vocab_size=1000,\n",
        "    state_dim=7,\n",
        "    action_dim=7,\n",
        "    embedding_dim=256,\n",
        "    fusion_dim=512,\n",
        "    num_action_bins=256,\n",
        "    fusion_type=\"attention\",\n",
        "    use_action_tokens=True\n",
        ")\n",
        "\n",
        "vla.eval()\n",
        "print(f\"Model created with {sum(p.numel() for p in vla.parameters()):,} parameters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create inputs\n",
        "image = torch.randn(1, 3, 224, 224)  # Random image\n",
        "instruction = \"pick up the red block\"\n",
        "tokens = tokenizer.encode(instruction).unsqueeze(0)\n",
        "state = torch.tensor([[0.5, -0.3, 0.8, 0.0, 0.2, -0.1, 1.0]])\n",
        "\n",
        "print(\"Inputs:\")\n",
        "print(f\"  Image: {image.shape}\")\n",
        "print(f\"  Instruction: '{instruction}'\")\n",
        "print(f\"  Robot state: {state[0].tolist()}\")\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    output = vla(image, tokens, state)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"OUTPUT ACTIONS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "actions = output['actions'][0].numpy()\n",
        "action_names = [\"joint_1\", \"joint_2\", \"joint_3\", \"joint_4\", \"joint_5\", \"joint_6\", \"gripper\"]\n",
        "\n",
        "for name, value in zip(action_names, actions):\n",
        "    bar_len = int((value + 1) * 20)\n",
        "    bar = \"█\" * max(0, bar_len)\n",
        "    print(f\"  {name:10s}: {value:+.3f} |{bar}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Instruction Sensitivity\n",
        "\n",
        "1. Does the (untrained) model respond differently to different instructions?\n",
        "\n",
        "**Key insight:** An *untrained* model produces random outputs. Only through training does it learn meaningful mappings!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different instructions\n",
        "\n",
        "test_instructions = [\n",
        "    \"pick up the red block\",\n",
        "    \"pick up the blue block\", \n",
        "    \"put down the object\",\n",
        "    \"move left\",\n",
        "    \"wave hello\"\n",
        "]\n",
        "\n",
        "print(\"Testing instruction sensitivity:\\n\")\n",
        "print(\"Note: This is an UNTRAINED model!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results = []\n",
        "\n",
        "for test_instruction in test_instructions:\n",
        "    test_tokens = tokenizer.encode(test_instruction).unsqueeze(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        test_output = vla(image, test_tokens, state)\n",
        "    \n",
        "    test_actions = test_output['actions'][0].numpy()\n",
        "    results.append(test_actions)\n",
        "    \n",
        "    print(f\"\\n'{test_instruction}'\")\n",
        "    print(f\"  Actions: {test_actions.round(2)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Similarity analysis (lower = more similar):\")\n",
        "for i in range(len(test_instructions)):\n",
        "    for j in range(i+1, len(test_instructions)):\n",
        "        diff = np.abs(results[i] - results[j]).mean()\n",
        "        print(f\"  '{test_instructions[i][:15]}' vs '{test_instructions[j][:15]}': {diff:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the VLA Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_vla_pipeline(model, img, toks, st, instr):\n",
        "    \"\"\"Visualize the VLA pipeline with embeddings and outputs.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        out = model(img, toks, st)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
        "    fig.suptitle(f\"VLA Pipeline: '{instr}'\", fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # Vision embedding\n",
        "    ax = axes[0, 0]\n",
        "    vision_emb = out['vision_embedding'][0].numpy()\n",
        "    ax.bar(range(50), vision_emb[:50], color='#FF6B6B', alpha=0.7)\n",
        "    ax.set_title('Vision Embedding', fontweight='bold')\n",
        "    ax.set_xlabel('Dimension')\n",
        "    \n",
        "    # Language embedding  \n",
        "    ax = axes[0, 1]\n",
        "    lang_emb = out['language_embedding'][0].numpy()\n",
        "    ax.bar(range(50), lang_emb[:50], color='#4ECDC4', alpha=0.7)\n",
        "    ax.set_title('Language Embedding', fontweight='bold')\n",
        "    ax.set_xlabel('Dimension')\n",
        "    \n",
        "    # State embedding\n",
        "    ax = axes[0, 2]\n",
        "    state_emb = out['state_embedding'][0].numpy()\n",
        "    ax.bar(range(50), state_emb[:50], color='#45B7D1', alpha=0.7)\n",
        "    ax.set_title('State Embedding', fontweight='bold')\n",
        "    ax.set_xlabel('Dimension')\n",
        "    \n",
        "    # Fused embedding\n",
        "    ax = axes[1, 0]\n",
        "    fused_emb = out['fused_embedding'][0].numpy()\n",
        "    ax.bar(range(100), fused_emb[:100], color='#9B59B6', alpha=0.7)\n",
        "    ax.set_title('Fused Embedding', fontweight='bold')\n",
        "    ax.set_xlabel('Dimension')\n",
        "    \n",
        "    # Actions\n",
        "    ax = axes[1, 1]\n",
        "    act = out['actions'][0].numpy()\n",
        "    names = ['J1', 'J2', 'J3', 'J4', 'J5', 'J6', 'Grip']\n",
        "    colors = ['#E74C3C' if a < 0 else '#27AE60' for a in act]\n",
        "    ax.barh(names, act, color=colors, alpha=0.7)\n",
        "    ax.set_title('Output Actions', fontweight='bold')\n",
        "    ax.set_xlim(-1.5, 1.5)\n",
        "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "    \n",
        "    # Architecture diagram\n",
        "    ax = axes[1, 2]\n",
        "    ax.text(0.5, 0.8, 'VLA Architecture', ha='center', fontsize=12, fontweight='bold')\n",
        "    ax.text(0.5, 0.6, 'Vision + Language + State', ha='center', fontsize=10)\n",
        "    ax.text(0.5, 0.45, '↓', ha='center', fontsize=14)\n",
        "    ax.text(0.5, 0.3, 'Fusion', ha='center', fontsize=10)\n",
        "    ax.text(0.5, 0.15, '↓', ha='center', fontsize=14)\n",
        "    ax.text(0.5, 0.0, 'Action Tokens', ha='center', fontsize=10)\n",
        "    ax.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_vla_pipeline(vla, image, tokens, state, instruction)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More Reflection Questions\n",
        "\n",
        "Before moving on, answer these questions with your lab partner:\n",
        "\n",
        "1. **Architecture Design:** Why do we convert all modalities to the same embedding dimension before fusion?\n",
        "\n",
        "2. **Training Data:** What kind of data would you need to train a VLA? How is this different from training a VLM? What are the potential challenges and safety concerns?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OPTIONAL Challenge Exercises\n",
        "\n",
        "### Challenge 1: Temporal Vision\n",
        "\n",
        "Real robots need to consider history. Modify the vision encoder to process a sequence of frames.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CHALLENGE 1: Temporal Vision Encoder\n",
        "\n",
        "class TemporalVisionEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Input: (batch, num_frames, channels, height, width)\n",
        "    Output: (batch, embedding_dim)\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim=256, num_frames=4):\n",
        "        super().__init__()\n",
        "        # TODO: Implement\n",
        "        # Hint: Process each frame with CNN, then use LSTM across time\n",
        "        pass\n",
        "    \n",
        "    def forward(self, image_sequence):\n",
        "        # TODO: Implement\n",
        "        pass\n",
        "\n",
        "# Test (uncomment when ready)\n",
        "# temporal_encoder = TemporalVisionEncoder()\n",
        "# image_seq = torch.randn(1, 4, 3, 224, 224)\n",
        "# temporal_emb = temporal_encoder(image_seq)\n",
        "# print(f\"Temporal embedding: {temporal_emb.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 2: Action Chunking\n",
        "\n",
        "Instead of predicting one action, predict a sequence of K future actions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CHALLENGE 2: Action Chunking\n",
        "\n",
        "class ChunkedActionDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Input: (batch, fusion_dim)\n",
        "    Output: (batch, chunk_size, action_dim)\n",
        "    \"\"\"\n",
        "    def __init__(self, fusion_dim=512, action_dim=7, chunk_size=10):\n",
        "        super().__init__()\n",
        "        self.chunk_size = chunk_size\n",
        "        self.action_dim = action_dim\n",
        "        # TODO: Implement\n",
        "        pass\n",
        "    \n",
        "    def forward(self, fused):\n",
        "        # TODO: Implement\n",
        "        pass\n",
        "\n",
        "# Test (uncomment when ready)\n",
        "# chunked = ChunkedActionDecoder(chunk_size=10)\n",
        "# action_chunk = chunked(fused_attention)\n",
        "# print(f\"Action chunk: {action_chunk.shape}\")  # Should be (1, 10, 7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Continue to\n",
        "[5_safety.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/5_safety.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "* [RT-2: Vision-Language-Action Models](https://robotics-transformer2.github.io/)\n",
        "* [OpenVLA](https://openvla.github.io/)\n",
        "* [SmolVLA](https://huggingface.co/blog/smolvla)\n",
        "* [LeRobot](https://github.com/huggingface/lerobot)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "blip",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
