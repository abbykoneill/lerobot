{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4edcef",
   "metadata": {},
   "source": [
    "# Introduction and Deployment of VLA: SMolVLA\n",
    "\n",
    "### Lab Table of Contents\n",
    "1. [1_chatgpt.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/1_chatgpt.ipynb)\n",
    "2. [2_CLIP.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/2_CLIP.ipynb)\n",
    "3. [3_VLM_BLIP.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/3_VLM_BLIP.ipynb)\n",
    "4. **[4_VLA.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/4_VLA.ipynb)**\n",
    "5. [5_safety.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/5_safety.ipynb)\n",
    "6. [Lab Checkoff](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/checkoff.txt)\n",
    "\n",
    "## VLA: Vision-Language Action Model\n",
    "Compared to VLMs, a VLA is specifically made for robotics where you want to be able to use language to help understand the correct action a robot should take.\n",
    "\n",
    "SmolVLA is an example of a VLA foundation model where inputs to the model are camera views, the robot's current state, and natural language instruction.\n",
    "\n",
    "For a more comprehensive overview on all LeRobot has to offer, [LeRobot Repo](https://learnopencv.com/vision-language-action-models-lerobot-policy/). \n",
    "\n",
    "#### Ensure you have a new environment Set-Up:\n",
    "* `conda create -n <env_name> python=3.10`\n",
    "\n",
    "Walk through each cell of the notebook to better understand the SmolVLA model. After running all cells, you should see an (image, action) pair. Reflect on the following questions.\n",
    "1. Does the action make sense based on the input image displayed? \n",
    "2. How clear or ambiguous is the action token? \n",
    "3. What challenges or safety concerns might we face if the output action token is ambiguous?\n",
    "4. How would you make the action token more clear? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a070fbef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "\n",
    "!pip install ipykernel matplotlib\n",
    "!pip install lerobot\n",
    "!conda install ffmpeg -c conda-forge -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3840c1",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a617a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "from lerobot.common.policies.lerobot_policy import LeRobotPolicy\n",
    "\n",
    "def show_image(img, instruction):\n",
    "    \"\"\"Display an image with its instruction.\"\"\"\n",
    "    plt.imshow(img.permute(1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n",
    "    plt.title(f\"Instruction: \\\"{instruction}\\\"\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1d5d7f",
   "metadata": {},
   "source": [
    "### Load the Pre-Trained VLA (SmolVLA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb68de8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the pre-trained SmolVLA policy\n",
    "# This downloads the model weights from the Hugging Face Hub\n",
    "policy = LeRobotPolicy.from_pretrained(\"lerobot/smolvla_base\").to(device)\n",
    "\n",
    "# Set the model to evaluation mode (disables things like dropout)\n",
    "policy.eval()\n",
    "\n",
    "print(\"SmolVLA model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c3979",
   "metadata": {},
   "source": [
    "### Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeff57a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset_repo_id = \"lerobot/svla_so100_pickplace\"\n",
    "dataset = LeRobotDataset(dataset_repo_id)\n",
    "\n",
    "print(f\"Loaded dataset with {len(dataset)} total steps.\")\n",
    "\n",
    "# Let's get a single sample from the dataset (e.g., sample #1000)\n",
    "sample_idx = 1000\n",
    "sample = dataset[sample_idx]\n",
    "\n",
    "# Let's see what's in our sample\n",
    "print(\"\\nSample keys:\")\n",
    "print(sample.keys())\n",
    "\n",
    "# The 'observation' key contains the inputs for the model\n",
    "print(\"\\nObservation keys:\")\n",
    "print(sample['observation'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aafe8d",
   "metadata": {},
   "source": [
    "### Prepare Inputs and Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1954f01",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get the instruction and observation from our sample\n",
    "instruction = sample['instruction']\n",
    "\n",
    "# Prepare the observation dictionary\n",
    "# We add a batch dimension (B) to each tensor, e.g., (C, H, W) -> (B, C, H, W)\n",
    "observation = {\n",
    "    'images.top': sample['observation']['images.top'].to(device).unsqueeze(0),\n",
    "    'state': sample['observation']['state'].to(device).unsqueeze(0)\n",
    "}\n",
    "\n",
    "# Run inference!\n",
    "# We use torch.no_grad() to tell PyTorch we're not training, which saves memory.\n",
    "with torch.no_grad():\n",
    "    action = policy.select_action(observation, instruction)\n",
    "\n",
    "print(f\"Instruction: \\\"{instruction}\\\"\")\n",
    "print(f\"\\nPredicted Action: {action.cpu().numpy()}\")\n",
    "\n",
    "# Let's also see what the \"ground truth\" action was (what the human did)\n",
    "ground_truth_action = sample['action'].numpy()\n",
    "print(f\"Ground Truth Action: {ground_truth_action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e4bc6",
   "metadata": {},
   "source": [
    "### Visualize the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d71aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "top_image = observation['images.top'][0].cpu()\n",
    "\n",
    "# Use our helper function to show the image and instruction\n",
    "show_image(top_image, instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3cba2",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [LeRobot VLA Demo](https://learnopencv.com/vision-language-action-models-lerobot-policy/)\n",
    "* [SmolVLA](https://smolvla.net/index_en)\n",
    "* [HuggingFace SmolVLA Blog](https://huggingface.co/blog/smolvla)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
