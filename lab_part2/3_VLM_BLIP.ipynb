{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc38577",
   "metadata": {},
   "source": [
    "# Introduction and Deployment of VLM: BLIP\n",
    "\n",
    "### Lab Table of Contents\n",
    "* Part 1\n",
    "    1. [1_imitation_learning.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part_1/1_imitation_learning.ipynb)\n",
    "* **Part 2**\n",
    "    1. [1_chatgpt.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/1_chatgpt.ipynb)\n",
    "    2. [2_CLIP.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/2_CLIP.ipynb)\n",
    "    3. **[3_VLM_BLIP.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/3_VLM_BLIP.ipynb)**\n",
    "    4. [4_VLA.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/4_VLA.ipynb)\n",
    "    5. [5_safety.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/5_safety.ipynb)\n",
    "* [Lab Checkoff](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/checkoff.md)\n",
    "\n",
    "## VLM: Vision-Language Model\n",
    "Compared to LLMs, a VLM is a generative AI model that takes input image and/or text and outputs text. \n",
    "\n",
    "![VLM Input/Output Diagram](robot_task_images/VLM_diagram.png)\n",
    "\n",
    "BLIP (Bootstrapped Language-Image Pretraining) is a VLM that has both image-text captioning and image-text retrieval capabilities. \n",
    "\n",
    "For this part of the lab, you will test a version of BLIP's image-text captioning as an introduction to how these models understand input images and reason/describe it to produce and output text caption.\n",
    "\n",
    "The below code implements BLIP through [HuggingFace](https://huggingface.co/docs/transformers/model_doc/blip). Walk through each cell of the notebook to test the VLM model on images from the Salesforce pretrained checkpoint, robot task images stored in the repository, and your own unique image.\n",
    "\n",
    "#### Ensure you have a new environment Set-Up:\n",
    "* `conda create -n <env_name> python=3.10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7d4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "\n",
    "!pip install ipykernel\n",
    "!pip install torch==2.9.1\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install pillow\n",
    "!pip install ipykernel\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7677afe4",
   "metadata": {},
   "source": [
    "### (A) Run BLIP Image-Captioning with Salesforce checkpoint image database\n",
    "\n",
    "Run the following cell to visualize the chosen input image from the Salesforce image base and the resulting model generated caption.\n",
    "\n",
    "1. Does the caption seem appropriate for the input image?\n",
    "2. Does the caption fully capture what is happening/depicted in the input image?\n",
    "3. How would you modify the caption to make it more descriptive of the image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae602c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image from Checkpoint\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load BLIP model & Processor from HuggingFace (https://huggingface.co/docs/transformers/model_doc/blip)\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# 2. Load demo image from URL\n",
    "image_url = \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\"\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "# 3. Display input image\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title('Input Image')\n",
    "plt.show()\n",
    "\n",
    "# 4. Prepare input image to feed into model\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# 5. Generate output caption\n",
    "out = model.generate(**inputs)\n",
    "print(\"Output Caption: \", processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a72a8f",
   "metadata": {},
   "source": [
    "### (B) Run BLIP Image-Captioning with Pre-chosen Robot Task Images\n",
    "\n",
    "Run the following cell to visualize the resulting model generated caption from one of the pre-saved robot task images in the repository. There are 5 pre-saved images. \n",
    "\n",
    "\\[Optional] To see the generated caption from a specific image in this directory, comment out the code to chose a random image from the directory and replace `selected_image` with the specific filename.\n",
    "\n",
    "1. Does the caption seem appropriate for the input image?\n",
    "2. Does the caption fully capture what is happening/depicted in the input image?\n",
    "3. How would you modify the caption to make it more descriptive of the image?\n",
    "\n",
    "Run the model several times to view the resulting captions for more than one of the pre-chosen images.\n",
    "\n",
    "4. How does the caption accuracy/fit compare between different images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a186464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository Image\n",
    "\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# 1. Load BLIP model & Processor from HuggingFace (https://huggingface.co/docs/transformers/model_doc/blip)\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# 2. Load pre-chosen robot task image from local file\n",
    "directory_path = 'robot_task_images/'\n",
    "extension = '*.png'\n",
    "image_files = glob.glob(f'{directory_path}{extension}')\n",
    "if image_files:\n",
    "    selected_image = random.choice(image_files)\n",
    "    print(f'Selected image from directory robot_task_images: {selected_image}')\n",
    "else:\n",
    "    print(f'No image files found in directory {directory_path}.')\n",
    "\n",
    "image = Image.open(selected_image).convert(\"RGB\")\n",
    "\n",
    "# 3. Display input image\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title('Input Image')\n",
    "plt.show()\n",
    "\n",
    "# 4. Prepare input image to feed into model\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# 5. Generate output caption\n",
    "out = model.generate(**inputs)\n",
    "print(\"Output Caption: \", processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456156e3",
   "metadata": {},
   "source": [
    "### (C) Run BLIP Image-Captioning with Custom Image(s)\n",
    "\n",
    "Run the following cell to visualize the resulting model generated caption from a new image of your choice by modifying the `image_path` string.\n",
    "\n",
    "1. Does the caption seem appropriate for the input image?\n",
    "2. Does the caption fully capture what is happening/depicted in the input image?\n",
    "3. How would you modify the caption to make it more descriptive of the image?\n",
    "\n",
    "Run the model as many times as you would like with new images to compare the results across images the model hasn't seen the checkpoint's training.\n",
    "\n",
    "4. How does the caption accuracy/fit compare between different images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351475b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Image\n",
    "\n",
    "# 1. Load BLIP model & Processor from HuggingFace (https://huggingface.co/docs/transformers/model_doc/blip)\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# 2. Load pre-chosen robot task image from local file\n",
    "image_path = 'path-to-your-image.png' # Replace with your image path\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# 3. Display input image\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title('Input Image')\n",
    "plt.show()\n",
    "\n",
    "# 4. Prepare input image to feed into model\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# 5. Generate output caption\n",
    "out = model.generate(**inputs)\n",
    "print(\"Output Caption: \", processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e82ec",
   "metadata": {},
   "source": [
    "### Continue to\n",
    "[4_VLA.ipynb](https://github.com/abbykoneill/lerobot/blob/main/lab_part2/4_VLA.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f02d6e",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [HuggingFace BLIP](https://huggingface.co/docs/transformers/model_doc/blip)\n",
    "* [Salesforce BLIP GitHub Repository](https://github.com/salesforce/BLIP)\n",
    "* [BLIP: Bootstrapping Langauge-Image Pre-training for Unified Vision Language Understanding and Generation - ArXiv](https://arxiv.org/abs/2201.12086)\n",
    "* [BLIP Blog](https://www.salesforce.com/blog/blip-bootstrapping-language-image-pretraining/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blip3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
